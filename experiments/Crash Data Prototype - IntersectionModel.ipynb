{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import psycopg2\n",
    "from geopy.distance import distance as geo_distance\n",
    "import geopandas as gpd\n",
    "# import shapely\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "# import time.strftime\n",
    "import matplotlib as mpl\n",
    "# import seaborn as sns\n",
    "mpl.rcParams['axes.linewidth'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bef13a",
   "metadata": {},
   "source": [
    "# Analysis idea:\n",
    "\n",
    "Loop over each intersection. Extract the num crashes within a certain radius. Calculate the number of crashes per yea for each unique intersection. Use a decision tree that uses a gini index on just the num-legs, angle data or some other simple model As we gather more quality feature data more sophisticated methods can replace the decision tree. Use the standard deviation of poisson distribution to calculate the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bdaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_DB= 'rws'\n",
    "POSTGRES_PASSWORD= 'ug_password'\n",
    "POSTGRES_USER= 'ug_username'\n",
    "CURRENT_DIR= os.getcwd()\n",
    "    \n",
    "conn = psycopg2.connect(f\"host=localhost dbname={POSTGRES_DB} user={POSTGRES_USER} password={POSTGRES_PASSWORD} port=5433\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae860405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur = conn.cursor()\n",
    "\n",
    "sql_crashes = f\"\"\"SELECT *,ST_AsText(dc.point) as t_point from crashes.dc_indexed as dc \"\"\"\n",
    "sql_ints = f\"\"\"SELECT * from planet_osm_intersections_alpha \"\"\"\n",
    "sql_roads = f\"\"\"SELECT * from planet_osm_roads\"\"\"\n",
    "\n",
    "# crashes = cur.fetchall()\n",
    "# df_int = pd.read_sql_query(sql_ints, conn)\n",
    "# df_crashes = pd.read_sql_query(sql_crashes, conn)\n",
    "df_int = gpd.read_postgis(sql_ints, conn, geom_col=\"point\")\n",
    "df_crashes = gpd.read_postgis(sql_crashes, conn,geom_col=\"point\")\n",
    "df_roads = gpd.read_postgis(sql_roads, conn, geom_col=\"way\")\n",
    "\n",
    "\n",
    "# cur.execute(f\"\"\"SELECT * , ST_Distance(ST_Transform(ST_SetSRID(ST_MakePoint({x},{y}), 4326),3857), xsect.point) as dist FROM planet_osm_intersections_alpha as xsect \n",
    "# WHERE  ST_Distance(ST_Transform(ST_SetSRID(ST_MakePoint({x},{y}), 4326),3857), xsect.point) < {search_radius} \n",
    "# ORDER BY dist\n",
    "#  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c3b94",
   "metadata": {},
   "source": [
    "# The above code pulls data from the postGIS database running in docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725decf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74770cc0",
   "metadata": {},
   "source": [
    "# Format datatypes and define exposure window\n",
    "The idea is that the crash data is collected from various sources. By looking at the data it seems there are fairly uniform collections from 2009-2022. So the strategy is going to use this time frame as our exposure time (time we are recording data in DC), and assume all intersection crashes are accurately recorded during this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x in df_crashes.columns])\n",
    "\n",
    "new_dtypes = {\"majorinjuries_bicyclist\": int,\n",
    "              \"majorinjuries_driver\": int,\n",
    "              \"majorinjuries_pedestrian\": int,\n",
    "              \"majorinjuriespassenger\": int,\n",
    "              \"fatal_driver\": int,\n",
    "              \"fatal_pedestrian\": int,\n",
    "              \"fatalpassenger\": int,\n",
    "              \"fatal_bicyclist\": int,\n",
    "              \"num_legs\": int\n",
    "             }\n",
    "df_crashes = df_crashes.astype(new_dtypes)\n",
    "# dataframe = dataframe.astype(new_dtypes)\n",
    "\n",
    "\n",
    "df_crashes['reportdate'] =  pd.to_datetime(df_crashes['reportdate'], format='%Y/%m/%d %H:%M:%S+%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crashes['reportdate'].hist(bins=150)\n",
    "# plt.yscale('log')\n",
    "print(df_crashes[(df_crashes['reportdate'] > \"2009\") &(df_crashes['reportdate'] < \"2022\") ]['reportdate'].min())\n",
    "print(df_crashes[(df_crashes['reportdate'] > \"2009\") &(df_crashes['reportdate'] < \"2022\") ]['reportdate'].max())\n",
    "print((df_crashes[(df_crashes['reportdate'] > \"2009\") &(df_crashes['reportdate'] < \"2022\") ]['reportdate'].max())-\\\n",
    "      (df_crashes[(df_crashes['reportdate'] > \"2009\") &(df_crashes['reportdate'] < \"2022\") ]['reportdate'].min()))\n",
    "print(\"We will normalize the crashes to an exposure time of 12.8 +- 0.5 years\")\n",
    "\n",
    "df_crashes[(df_crashes['reportdate'] > \"2009\") &(df_crashes['reportdate'] < \"2022\") ]['reportdate'].hist(bins=150)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8308a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_time = 12.8\n",
    "exposure_time_up = 13.3\n",
    "exposure_time_down = 12.3\n",
    "df_crashes = df_crashes[(df_crashes['reportdate'] > \"2009\") &(df_crashes['reportdate'] < \"2022\") ]\n",
    "# severe_columns = [x for x in df_crashes.columns if \"FATAL\" in x.upper() or \"MAJOR\" in x.upper()]\n",
    "# df_crashes_severe = df_crashes[ pd.DataFrame.any(df_crashes[severe_columns].astype(int) > 0,axis=1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea88bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_crashes_fatal['crash_count'] = 0\n",
    "major_injury_columns = [x for x in df_crashes.columns if \"MAJOR\" in x.upper()]\n",
    "fatal_injury_columns = [x for x in df_crashes.columns if \"FATAL\" in x.upper()]\n",
    "# df_crashes[ pd.DataFrame.any(df_crashes[severe_columns].astype(int) > 0,axis=1) ]\n",
    "major_injury_columns\n",
    "fatal_injury_columns\n",
    "# print(df_int.shape)\n",
    "# print(df_crashes_fatal.shape)\n",
    "# for i,row in enumerate(df_int.geometry):\n",
    "#     print(i)\n",
    "#     df_int.loc[i,'crash_count'] = sum(df_crashes_fatal.geometry.distance(row) < 50)\n",
    "# severe_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c1ae0",
   "metadata": {},
   "source": [
    "# Major calculation section of the notebook - associating crashes to intersections\n",
    "Loop over intersections and calculate the crash rates for all, severe, and fatal crashes per intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8381b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_int['crash_rate'] = 0\n",
    "df_int['major_injury_crash_rate'] = 0\n",
    "df_int['fatal_crash_rate'] = 0\n",
    "\n",
    "df_int['distance_weighted_crash_count'] = 0\n",
    "df_int['involvesBike'] = 0\n",
    "# df_crashes_severe = df_crashes_severe.sample(2500)\n",
    "# df_int = df_int.sample(2500)\n",
    "\n",
    "\n",
    "print(df_int.shape)\n",
    "# print(df_crashes_severe.shape)\n",
    "print(df_crashes.shape)\n",
    "crash_buffer = df_crashes.geometry.buffer(50)\n",
    "\n",
    "for i,row in tqdm(enumerate(df_int.geometry)):\n",
    "#     print(i)\n",
    "#     df_crashes.geometry.buffer(50)\n",
    "#     pdb.set_trace()\n",
    "    buffer_index = crash_buffer.contains(row)\n",
    "#     dist_vector = df_crashes.geometry.distance(row).astype(float)\n",
    "#     dist_vector = dist_vector.fillna(1000000)\n",
    "    df_int.loc[i,'crash_rate'] = sum(buffer_index) / exposure_time\n",
    "    df_int.loc[i,'major_injury_crash_rate'] = len(df_crashes[((buffer_index) & (df_crashes[major_injury_columns].astype(bool).any(axis=1)))]) / exposure_time\n",
    "    df_int.loc[i,'fatal_crash_rate'] = len(df_crashes[((buffer_index) & (df_crashes[fatal_injury_columns].astype(bool).any(axis=1)))]) / exposure_time\n",
    "\n",
    "#     pdb.set_trace()\n",
    "#     if i > 5:\n",
    "#         break\n",
    "#     if sum(dist_vector < 50):\n",
    "#         pdb.set_trace()\n",
    "#         df_int.loc[i,'distance_weighted_crash_count'] = sum( (dist_vector < 50).apply(int)*(10/(dist_vector+0.0001)) )\n",
    "    \n",
    "#     df_int.loc[i,'isFatal'] = sum( (dist_vector < 50).apply(int)*(df_crashes_severe['fatal_bicyclist'].astype(int)+df_crashes_severe['fatal_driver'].astype(int)+df_crashes_severe['fatal_pedestrian'].astype(int)+df_crashes_severe['fatalpassenger'].astype(int)) )\n",
    "    \n",
    "df_int['crash_rate_exposure_err_up'] = df_int['crash_rate'] * (exposure_time/exposure_time_up)\n",
    "df_int['major_injury_crash_rate_exposure_err_up'] = df_int['major_injury_crash_rate'] * (exposure_time/exposure_time_up)\n",
    "df_int['fatal_crash_rate_exposure_err_up'] = df_int['fatal_crash_rate'] * (exposure_time/exposure_time_up)\n",
    "\n",
    "df_int['crash_rate_stat_err'] = np.sqrt(df_int['crash_rate']*exposure_time)/exposure_time\n",
    "df_int['major_injury_crash_rate_stat_err'] = np.sqrt(df_int['major_injury_crash_rate']*exposure_time)/exposure_time\n",
    "df_int['fatal_crash_rate_stat_err'] = np.sqrt(df_int['fatal_crash_rate']*exposure_time)/exposure_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An uncertainty of 0 for poisson statistics is undefined, assume 1 accident over the exposure time: sqrt(1)/12.8\n",
    "df_int['crash_rate_stat_err'] = df_int['crash_rate_stat_err'].apply(lambda x: x if x>0 else 0.08)\n",
    "df_int['fatal_crash_rate_stat_err'] = df_int['fatal_crash_rate_stat_err'].apply(lambda x: x if x>0 else 0.08)\n",
    "df_int['major_injury_crash_rate_stat_err'] = df_int['major_injury_crash_rate_stat_err'].apply(lambda x: x if x>0 else 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_int.head(50)\n",
    "df_int.to_csv('df_int_before_modeling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c494c",
   "metadata": {},
   "source": [
    "# Second major calculation - extract road type features from OSM table\n",
    "This would be much easier if we just had that information in the flat intersection table. We also do some formatting to convert \"num_legs\" and \"road_type\" into categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5906b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_int['road_type'] = None\n",
    "\n",
    "from shapely.geometry import Point, box\n",
    "\n",
    "def most_common(lst):\n",
    "    if not lst:\n",
    "        return \"service_road\"\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "for i,row in zip(df_int.geometry.index, df_int.geometry):\n",
    "    print(i)\n",
    "\n",
    "    #s.sindex.nearest(Point(1, 1))\n",
    "    dist_v = df_roads.geometry.distance(row).astype(float)\n",
    "#     print( most_common( list(df_roads[dist_v < 50]['highway'].values)))\n",
    "    df_int.loc[i,'road_type'] = most_common( list( df_roads[dist_v < 50]['highway'].values ))\n",
    "    \n",
    "    \n",
    "#     if i > 500 : break    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ad000",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtypes = ['motorway', \"service_road\", 'trunk', 'motorway_link', 'primary',\n",
    "       'primary_link', 'secondary', 'secondary_link', 'path', 'cycleway',\n",
    "       'trunk_link', 'footway', 'construction']\n",
    "\n",
    "for rtype in rtypes:\n",
    "    df_int[rtype] = 0\n",
    "    \n",
    "for i,row in df_int.iterrows():\n",
    "    df_int.loc[i,row['road_type']] = 1\n",
    "    \n",
    "legs = ['2_leg','3_leg','4_leg','5_leg','many_leg']\n",
    "for leg_type in legs:\n",
    "    df_int[leg_type] = 0\n",
    "    \n",
    "for i,row in df_int.iterrows():\n",
    "    \n",
    "    if int(float(df_int.loc[i,'num_legs'])) == 2:\n",
    "        df_int.loc[i,'2_leg'] = 1\n",
    "    elif int(float(df_int.loc[i,'num_legs'])) == 3:\n",
    "        df_int.loc[i,'3_leg'] = 1\n",
    "    elif int(float(df_int.loc[i,'num_legs'])) == 4:\n",
    "        df_int.loc[i,'4_leg'] = 1\n",
    "    elif int(float(df_int.loc[i,'num_legs'])) == 5:\n",
    "        df_int.loc[i,'5_leg'] = 1\n",
    "    else:\n",
    "        df_int.loc[i,'many_leg'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bead82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_int.loc[good_index][['num_legs_from_borderalgo','angle','oneway']+rtypes]\n",
    "df_int['crash_count'] = df_int['crash_rate'] *12.8\n",
    "df_int['major_injury_crash_count'] = df_int['major_injury_crash_rate']*12.8\n",
    "df_int['fatal_crash_count'] = df_int['fatal_crash_rate']*12.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int['num_legs'].isna().any()\n",
    "df_int['crash_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba21c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int['crash_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55ca57",
   "metadata": {},
   "source": [
    "# Simple statistical analysis\n",
    "Fit to a tweedie distribution. Convert angle to cosine(rads). Don't have to worry too much about overfitting, because N >> M (much more data than parameters -> deterministic solution). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05217289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "import pdb\n",
    "\n",
    "def cast_to_float(row):\n",
    "    if row:\n",
    "        return float(row)\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "\n",
    "clf = linear_model.TweedieRegressor(power=1.1, alpha=0.015, fit_intercept=True, link='log')\n",
    "\n",
    "# good_index = df_int['crash_count'].apply(cast_to_float).dropna().index\n",
    "# df_int['distance_weighted_crash_count'] = df_int['distance_weighted_crash_count'].apply(float).fillna(0)\n",
    "# df_int['num_legs_from_borderalgo'] = df_int['num_legs_from_borderalgo'].apply(cast_to_float).fillna(4)\n",
    "# df_int['angle'] = df_int['angle'].apply(cast_to_float).fillna(0)\n",
    "# df_int['oneway'] = df_int['oneway'].apply(cast_to_float).fillna(1)\n",
    "\n",
    "good_index = df_int.index\n",
    "# good_index = df_int['crash_count'].apply(cast_to_float).dropna().index\n",
    "print(good_index)\n",
    "# df_int.loc[good_index]['num_legs_from_borderalgo'] = df_int.loc[good_index]['num_legs_from_borderalgo'].fillna(4)\n",
    "# good_index = df_int.loc[good_index]['num_legs'].apply(cast_to_float).dropna().index\n",
    "# good_index = df_int.loc[good_index].dropna()\n",
    "print(good_index.shape)\n",
    "Y = df_int.loc[good_index,'crash_count']#.apply(float)\n",
    "# Y = df_int.loc[good_index,'crash_'].apply(float)\n",
    "\n",
    "\n",
    "X = df_int.loc[good_index][['num_legs','angle','oneway']+rtypes+legs].astype(float)\n",
    "X['angle'] = np.cos(X['angle'] * (np.pi/180))\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "X['const'] = 1\n",
    "X_train = X[:15000]\n",
    "Y_train = Y[:15000]\n",
    "X_test = X[15000:]\n",
    "Y_test = Y[15000:]\n",
    "# scaler.fit(X_train[['num_legs_from_borderalgo','angle','oneway']])\n",
    "# X_train[['num_legs_from_borderalgo','angle','oneway']] = scaler.transform(X_train[['num_legs_from_borderalgo','angle','oneway']])\n",
    "# X_test[['num_legs_from_borderalgo','angle','oneway']] = scaler.transform(X_test[['num_legs_from_borderalgo','angle','oneway']])\n",
    "\n",
    "\n",
    "# pdb.set_trace()\n",
    "print(X_test.shape)\n",
    "\n",
    "# print(len(X))\n",
    "# clf.fi\n",
    "result = clf.fit(X_train, Y_train)\n",
    "print(\"Train score : \" + str(clf.score(X_train, Y_train)))\n",
    "print(\"Test score : \" + str(clf.score(X_test, Y_test)))\n",
    "\n",
    "# print(X.columns)\n",
    "print(result.coef_)\n",
    "# print(clf.predict(X_test))\n",
    "# plt.hist(clf.predict(X_test),bins=10)\n",
    "# plt.show()\n",
    "# print(clf.predict(X_test).std())\n",
    "# print(clf.predict(X_test))\n",
    "plt.hist(Y_test-clf.predict(X_test),bins=25)\n",
    "resids = Y_test-clf.predict(X_test)\n",
    "\n",
    "best_fit_line = scipy.stats.norm.pdf(np.linspace(-100,300,1000), resids.mean(), resids.std(ddof=1))*8000\n",
    "\n",
    "plt.plot(np.linspace(-100,300,1000), best_fit_line)\n",
    "# plt.yscale('log')\n",
    "# plt.xlim(-5,30)\n",
    "plt.title(\"Residuals of truth-prediction\")\n",
    "plt.show()\n",
    "print(\"std: \" + str((Y_test-clf.predict(X_test)).std(ddof=1)))\n",
    "print(\"mean: \" + str((Y_test-clf.predict(X_test)).mean() ))\n",
    "plt.hist(Y_test - Y_test.mean(),bins=35)\n",
    "plt.title(\"Residuals using mean as prediction.\")\n",
    "plt.show()\n",
    "# print((Y_test - Y_test.mean()).mean())\n",
    "print(\"std using mean as prediction :\" +str((Y_test - Y_test.mean()).std(ddof=1)))\n",
    "\n",
    "plt.hist(Y_train-clf.predict(X_train),bins=35)\n",
    "plt.title('Residuals of truth-prediction for training data')\n",
    "# plt.yscale('log')\n",
    "# plt.xlim(-5,30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7408efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5389aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_test,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20698034",
   "metadata": {},
   "source": [
    "# I ad-hoc create the AI prediction columns in the df_int dataframe for export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_int['AI_fatal_crash_rate'] = clf.predict(X)/12.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_int['AI_fatal_rate_err'] = 0.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6547711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_int.to_csv(\"Intersections_withCrashRates_WithAI_Preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f37dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x for x in clf.predict(X_test)]\n",
    "# X_test['']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8355b9d1",
   "metadata": {},
   "source": [
    "# Only scratchpad work below this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,10))\n",
    "bins = plt.hist( [ x*(10/12.8) for x in Y_test ] , rwidth=0.95)\n",
    "# plt.xlim(1,6)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"# of major injury crashes in 10 years\",fontsize=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8efa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['prediction'] = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,10))\n",
    "(X_test['prediction']*(10/12.8)).hist(bins=8,rwidth=0.95)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Predicted # major injury crashes in 10 years\",fontsize=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069673ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_safe = X_test[ X_test['prediction'] < 20 ]\n",
    "X_danger = X_test[ X_test['prediction'] > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8dcf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_safe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc827276",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_danger.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a significant diference between angles?\n",
    "# Z = (mu_1 - mu_2 / sqrt(sigma_1^2 + sigma_2^2))\n",
    "Z = (1.525520 - 1.477102) / np.sqrt(0.460798**2 + 0.177618**2)\n",
    "print(Z)\n",
    "# No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17178222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int['num_legs_from_borderalgo'].apply(cast_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33086bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(df_int['oneway'].apply(cast_to_float),df_int['crash_count'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018222c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int.to_csv('crash_model_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pd.DataFrame()\n",
    "# severe_columns = [x for x in df_crashes.columns if \"FATAL\" in x.upper() or \"MAJOR\" in x.upper()]\n",
    "# df_crashes_fatal = df_crashes[ pd.DataFrame.any(df_crashes[severe_columns].astype(int) > 0,axis=1) ]\n",
    "\n",
    "#     df_int_dict = df_int.to_dict('records')\n",
    "# df_crashes_dict = df_crashes_fatal.to_dict('records')\n",
    "\n",
    "# crash_mapping = []\n",
    "\n",
    "# print(\"Beginning loop...\")\n",
    "# for i, intersection in enumerate(df_int_dict):\n",
    "#     crash_count = 0\n",
    "#     crash_ids = []\n",
    "#     for j, crash in enumerate(df_crashes_dict):\n",
    "#         distance = geo_distance((intersection['latitude'],intersection['longitude']),\n",
    "#                                 (crash['latitude'],crash['longitude']))\n",
    "        \n",
    "#         radius = distance.m\n",
    "        \n",
    "#         if radius < I 50:\n",
    "#             crash_count += 1\n",
    "#             crash_ids.append(crash['objectid'])\n",
    "#     print(\"Intersection #: \" + str(i))\n",
    "#     print(\"crash_count:\" + str(crash_count))\n",
    "#     crash_mapping.append((intersection['nodeid'],crash_ids)) \n",
    "    \n",
    "#     if i > 1000:\n",
    "#         break\n",
    "    \n",
    "# crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0439b4",
   "metadata": {},
   "source": [
    "# Below is from an earlier exploration on poisson fits\n",
    "The data is two skewed to fit to a poisson. Once the data is normalized by traffic volume this may be worth revisiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31773d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_severe(row):\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f956d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ll_poisson(y, X, beta, alph):\n",
    "    \"\"\"\n",
    "    Poisson = (lambda^N*exp(-lambda))/N!\n",
    "    \"\"\"\n",
    "    mu = np.exp(np.dot(X, beta))\n",
    "    size = 1/alph\n",
    "    prob = size/(size+mu)\n",
    "    ll = nbinom.logpmf(y, size, prob)\n",
    "    ll = poisson.logpmf(y,)\n",
    "    return ll\n",
    "\n",
    "class Poisson(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, exog, **kwds):\n",
    "        super(NBin, self).__init__(endog, exog, **kwds)\n",
    "\n",
    "    def nloglikeobs(self, params):\n",
    "        alph = params[-1]\n",
    "        beta = params[:-1]\n",
    "        ll = _ll_nb2(self.endog, self.exog, beta, alph)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, start_params=None, maxiter=10000, maxfun=5000, **kwds):\n",
    "        # we have one additional parameter and we need to add it for summary\n",
    "        self.exog_names.append('alpha')\n",
    "        if start_params == None:\n",
    "            # Reasonable starting values\n",
    "            start_params = np.append(np.zeros(self.exog.shape[1]), .5)\n",
    "            # intercept\n",
    "            start_params[-2] = np.log(self.endog.mean())\n",
    "        return super(NBin, self).fit(start_params=start_params,\n",
    "                                     maxiter=maxiter, maxfun=maxfun,\n",
    "                                     **kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dba8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import factorial\n",
    "from scipy import stats\n",
    "import copy\n",
    "\n",
    "def poisson(k, lamb):\n",
    "    \"\"\"poisson pdf, parameter lamb is the fit parameter\"\"\"\n",
    "    return (lamb**k/factorial(k)) * np.exp(-lamb)\n",
    "\n",
    "\n",
    "def negative_log_likelihood(params, data):\n",
    "    \"\"\"\n",
    "    The negative log-Likelihood-Function\n",
    "    \"\"\"\n",
    "\n",
    "    lnl = - np.sum(np.log(poisson(data, params[0])))\n",
    "    return lnl\n",
    "\n",
    "def negative_log_likelihood(params, data):\n",
    "    ''' better alternative using scipy '''\n",
    "    return -stats.poisson.logpmf(data, params[0]).sum()\n",
    "\n",
    "\n",
    "# get poisson deviated random numbers\n",
    "# data = np.random.poisson(1.2, 1000)\n",
    "data = df_int[df_int['num_legs'].apply(cast_to_float)==4]['crash_count'].apply(cast_to_float).dropna()\n",
    "print(data)\n",
    "\n",
    "# minimize the negative log-Likelihood\n",
    "\n",
    "result = minimize(negative_log_likelihood,  # function to minimize\n",
    "                  x0=np.ones(1),            # start value\n",
    "                  args=(data,),             # additional arguments for function\n",
    "                  method='Powell',          # minimization method, see docs\n",
    "                  )\n",
    "# result is a scipy optimize result object, the fit parameters \n",
    "# are stored in result.x\n",
    "print(result)\n",
    "# print(dir(result))\n",
    "func_min = result.fun\n",
    "\n",
    "scan_value = func_min\n",
    "scan_parameter = copy.deepcopy(result.x)\n",
    "while scan_value < 2*func_min:\n",
    "    scan_value = negative_log_likelihood(scan_parameter,data)\n",
    "    scan_parameter[0] += 0.2\n",
    "print(\"1Sigma value is :\" )\n",
    "print(scan_parameter)\n",
    "print(scan_value)\n",
    "    \n",
    "# plot poisson-distribution with fitted parameter\n",
    "x_plot = np.arange(0, 35)\n",
    "\n",
    "plt.plot(\n",
    "    x_plot,\n",
    "    stats.poisson.pmf(x_plot, result.x[0]),\n",
    "    marker='o', linestyle='',\n",
    "    label='Fit result',\n",
    ")\n",
    "plt.plot(\n",
    "    x_plot,\n",
    "    stats.poisson.pmf(x_plot, scan_parameter[0]),\n",
    "    marker='x', linestyle='',\n",
    "    label='Uncertainty result'\n",
    ")\n",
    "plt.hist(df_int['crash_count'].apply(cast_to_float),density=True,bins=35,label='Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.poisson.pmf(x_plot, scan_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df_int['crash_count'].apply(cast_to_float).dropna().index\n",
    "df_int.loc[index,'crash_count'].apply(cast_to_float)\n",
    "df_int.loc[index][['num_legs_from_borderalgo','angle']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(clf.family.unit_variance(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13df292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweedie, seaborn as sns, matplotlib.pyplot as plt\n",
    "\n",
    "mu = 3.5\n",
    "phi = np.sqrt(clf.family.unit_variance(mu)/mu**1.5)\n",
    "phi = np.sqrt(clf.family.unit_variance(mu))\n",
    "\n",
    "\n",
    "tvs = tweedie.tweedie(mu=mu, p=1.5, phi=phi).rvs(10000)\n",
    "plt.hist(tvs,bins=50,density=True)\n",
    "# plt.yscale('log')\n",
    "# ax = sns.kdeplot(tvs,bw=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c23fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = tweedie.tweedie.rvs(1.5,5,3,size=40)\n",
    "results = tweedie.tweedie.fit(rvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7948796",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( tweedie.tweedie.pdf(np.linspace(0,100,num=60),results[-1],results[1],results[2]) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( tweedie.tweedie.pdf(np.linspace(0,100,num=60),1.5,5,3) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b25a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
